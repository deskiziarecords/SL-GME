{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SL-GME: Initial Semantic Load Analysis\n",
    "\n",
    "This notebook demonstrates the core semantic load calculation.\n",
    "\n",
    "## Theory\n",
    "\n",
    "We compute semantic load as:\n",
    "\n",
    "$$Λ(ℓ) = I_{concept}(ℓ) - I_{surface}(ℓ)$$\n",
    "\n",
    "Where:\n",
    "- $I_{concept}(ℓ)$ = information token $ℓ$ contributes to preserving definitions/concepts\n",
    "- $I_{surface}(ℓ)$ = information token $ℓ$ contributes to general text fluency\n",
    "\n",
    "## Intuition\n",
    "- <followup encodedFollowup="%7B%22snippet%22%3A%22High%20%CE%9B%3A%20Concept-critical%20tokens%20(like%20'z'%20in%20'zero%2C%20zenith%2C%20zen')%22%2C%22question%22%3A%22Can%20you%20provide%20more%20examples%20of%20high-%CE%9B%20tokens%20and%20explain%20why%20they%20are%20considered%20concept-critical%3F%22%2C%22id%22%3A%2279ec7ff5-c284-46c2-8a5f-155338a01152%22%7D" />\n",
    "- <followup encodedFollowup="%7B%22snippet%22%3A%22Low%20%CE%9B%3A%20Predictable%20tokens%20(like%20'e'%20in%20'the%2C%20be%2C%20we')%22%2C%22question%22%3A%22Why%20are%20predictable%20tokens%20like%20'e'%20in%20'the%2C%20be%2C%20we'%20classified%20as%20low-%CE%9B%2C%20and%20how%20does%20this%20impact%20model%20evolution%3F%22%2C%22id%22%3A%2228b43ebc-1a4c-42e9-ae76-6506dc5a7fb9%22%7D" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src')\n",
    "\n",
    "from semantic_load.calculator import SemanticLoadCalculator\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize calculator\n",
    "calculator = SemanticLoadCalculator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data\n",
    "lambda_values = calculator.analyze(\n",
    "    corpus_file=\"../../data/corpora/wikipedia_sample.txt\",\n",
    "    concepts_file=\"../../data/concepts/atomic_concepts.json\"\n",
    ")\n",
    "\n",
    "# Display top and bottom tokens\n",
    "print(\"Top 10 high-semantic tokens:\")\n",
    "for token, value in list(lambda_values.items())[:10]:\n",
    "    print(f\" {token}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nTop 10 low-semantic tokens:\")\n",
    "for token, value in list(lambda_values.items())[-10:]:\n",
    "    print(f\" {token}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Histogram\n",
    "plt.subplot(1, 2, 1)\n",
    "values = list(lambda_values.values())\n",
    "plt.hist(values, bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Semantic Load Λ(ℓ)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Semantic Load')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot (sorted)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(range(len(values)), sorted(values), alpha=0.6, s=10)\n",
    "plt.xlabel('Token Rank')\n",
    "plt.ylabel('Semantic Load')\n",
    "plt.title('Semantic Load by Token Rank')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze token categories\n",
    "def categorize_token(token):\n",
    "    \"\"\"Simple token categorization\"\"\"\n",
    "    if token in ['mathematics', 'physics', 'biology', 'chemistry',\n",
    "                'philosophy', 'linguistics', 'psychology', 'economics']:\n",
    "        return 'Domain Term'\n",
    "    elif token in ['the', 'and', 'of', 'in', 'to', 'is', 'for', 'with']:\n",
    "        return 'Function Word'\n",
    "    elif token in ['study', 'analyzes', 'explains', 'examines', 'investigates']:\n",
    "        return 'Process Verb'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Calculate average semantic load by category\n",
    "categories = {}\n",
    "for token, value in lambda_values.items():\n",
    "    category = categorize_token(token)\n",
    "    if category not in categories:\n",
    "        categories[category] = []\n",
    "    categories[category].append(value)\n",
    "\n",
    "# Plot by category\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_names = []\n",
    "category_means = []\n",
    "\n",
    "for category, values in categories.items():\n",
    "    if len(values) > 1:  # Only show categories with multiple tokens\n",
    "        category_names.append(category)\n",
    "        category_means.append(sum(values) / len(values))\n",
    "\n",
    "bars = plt.bar(category_names, category_means, alpha=0.7)\n",
    "plt.xlabel('Token Category')\n",
    "plt.ylabel('Average Semantic Load')\n",
    "plt.title('Semantic Load by Token Category')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean in zip(bars, category_means):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "             f'{mean:.2f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights\n",
    "\n",
    "From this analysis, we should observe:\n",
    "\n",
    "1. <followup encodedFollowup="%7B%22snippet%22%3A%22Domain%20terms%20(mathematics%2C%20physics%2C%20etc.)%20have%20high%20semantic%20load%22%2C%22question%22%3A%22How%20does%20the%20semantic%20load%20of%20domain-specific%20terms%20compare%20across%20different%20fields%20like%20mathematics%2C%20physics%2C%20and%20biology%3F%22%2C%22id%22%3A%2293926a9e-83e1-43a5-83a8-f0002a047646%22%7D" />\n",
    "2. <followup encodedFollowup="%7B%22snippet%22%3A%22Function%20words%20(the%2C%20and%2C%20of)%20have%20low%20semantic%20load%22%2C%22question%22%3A%22What%20role%20do%20function%20words%20play%20in%20language%20models%20despite%20their%20low%20semantic%20load%3F%22%2C%22id%22%3A%228f6b3faa-5f9b-40fe-81f9-62d420c6162d%22%7D" />\n",
    "3. <followup encodedFollowup="%7B%22snippet%22%3A%22Process%20verbs%20(study%2C%20analyze)%20have%20moderate%20semantic%20load%22%2C%22question%22%3A%22Why%20do%20process%20verbs%20like%20'study'%20and%20'analyze'%20have%20a%20moderate%20semantic%20load%2C%20and%20how%20does%20this%20affect%20their%20treatment%20in%20model%20evolution%3F%22%2C%22id%22%3A%22d10566ee-c048-4820-9b90-1c55e593cba5%22%7D" />\n",
    "\n",
    "This validates our core hypothesis: some tokens do more \"semantic work\" than others."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
